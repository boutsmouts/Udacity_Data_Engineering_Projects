# Project 1: Data Modeling with Postgres

## Introduction
This project is part of the Udacity Data Engineer Nano Degree Program. The startup "Sparkify" wants a database to analyze collected user and song data stored locally as JSON files. Their particular interest lies on what songs their customers are listening to, which is why they need an easy to query database.

The task is to build a PostgreSQL database schema and an ETL pipeline to achieve the analysis.

## Datasets

The collected data consist of mainly two file types both given as JSON files:

1. Log data, which provide user activity logs from a music streaming app (generated by this [Event Simulator](https://github.com/Interana/eventsim))
2. Song data, which provide metadata about a song and its artist (subset of real data from the [Million Song Dataset](http://millionsongdataset.com/))

### Schema for Song Play Analysis

The preferred schema for this analysis is a star schema with one fact table and four dimension tables in total.

#### Fact Table

1. songplays: holds log data associated with song plays filtered using 'NextSong' for page
    - songplay_id (SERIAL - PRIMARY KEY): Unique ID for song plays
    - start_time (TIMESTAMP): Timestamp when song play was started (refers to table time - see below)
    - user_id (INT): User ID (refers to table users - see below)
    - level (VARCHAR): Tier of user
    - song_id (VARCHAR): Song ID (refers to table songs - see below)
    - artist_id (VARCHAR): Artist ID (refers to table artists - see below)
    - session_id (INT): Session ID of users
    - location (VARCHAR): Location of users
    - user_agent (VARCHAR): Program used by users to access song play

#### Dimension Tables

1. users: holds user data in app
    - user_id (INT - PRIMARY KEY): Unique ID for users
    - firt_name (VARCHAR): First name of user
    - last_name (VARCHAR): Last name of user
    - gender (VARCHAR): Gender of user
    - level (VARCHAR): Tier of user


2. songs: holds songs in music library
    - song_id (INT - PRIMARY KEY): Unique ID for songs
    - title (VARCHAR): Title of song
    - artist_id (VARCHAR): Artist ID (refers to table artists - see below)
    - year (INT): Year of song
    - duration (FLOAT): Length of song


3. artists: holds artists in music library
    - artist_id (VARCHAR - PRIMARY KEY): Unique ID for artists
    - name (VARCHAR): Name of artist
    - location (VARCHAR): Location of artist
    - latitude (FLOAT): Geographical latitude of location
    - longitude (FLOAT): Geographical longitude of location


4. time: holds timestamps of song plays separated into specific units
    - start_time (TIMESTAMP - PRIMARY KEY): Unique timestamp of song play
    - hour (INT): Hour of timestamp
    - day (INT): Day of timestamp
    - week (INT): Number of week in year
    - month (INT): Month of timestamp
    - year (INT): Year of timestamp
    - weekday (INT): Numeric value for the day of the week in timestamp

## Program Structure

The following files are included for this project:

  1. 'sql_queries.py', used to store necessary SQL queries to create, fill and query the tables
  2. 'create_tables.py', used to create the database and relevant tables; requires 'sql_queries.py'
  3. 'etl.py', used to read, process and store the given JSON song and log files in the database; requires 'sql_queries.py'
  4. 'etl.ipynb', used to read and process a single file from JSON song and log data (for introductory purposes)
  5. 'test.ipynb', used to test the implemented tables in the database with sample queries

Mandatory python modules to run the scripts:

- psycopg2
- os
- glob
- pandas

Note: The solution to the project has been developed locally using only the python files ('.py') provided and no Jupyter Notebooks ('.ipynb'). Developed using Python 3.6.10.

#### Instructions to run the program

Use the following steps to create the desired database and tables for the project. Repeat if necessary.

1. Run 'create_tables.py' in your terminal using: 'python create_tables.py'
2. Run 'etl.py' in your terminal using: 'python etl.py'

Note: For successfully running these two scripts, you may need to adjust your 'PWD' of your terminal to the folder where 'create_tables.py', 'sql_queries.py', and 'etl.py' are stored.
