# Project 5: Data Pipelines with Apache Airflow on AWS

## Introduction
This project is part of the Udacity Data Engineer Nano Degree Program. The music streaming startup "Sparkify" has further grown and now wants a maintainable and scalable pipeline to stage, store and query their user and song data on Amazon Web Services (AWS). Their data are stored as JSON files in S3 buckets on AWS. Their particular interest lies on what songs their customers are listening to, which is why they need an easy to query database.

The task is to implement an Apache Airflow DAG that consist of multiple operators to stage the data from S3 buckets to Redshift, load the staged data into fact and dimension tables and perform a data quality check to confirm that the data are present.

## Datasets

The collected data consist of mainly two file types both given as JSON files:

1. Log data, which provide user activity logs from a music streaming app (generated by this [Event Simulator](https://github.com/Interana/eventsim))
2. Song data, which provide metadata about a song and its artist (subset of real data from the [Million Song Dataset](http://millionsongdataset.com/))

Log data are stored under: ``S3://udacity-dend/log_data/*/*/*.json``
<br>Song data are stored under: ``S3://udacity-dend/song_data/*/*/*/*.json``
<br>Log mapping file is stored under: ``S3://udacity-dend/log_json_path.json``

### Schema for Song Play Analysis

Airflow calls multiple operators that stages the data from S3 buckets into two staging tables. From there on, data are loaded into one fact table and four dimension tables.

#### Staging Tables

1. ``staging_events``: holds all log data
2. ``staging_songs``: holds all song data

#### Fact Table

1. ``songplays``: holds log data associated with song plays filtered using ``'NextSong'`` for page

#### Dimension Tables

1. ``users``: holds user data in app
2. ``songs``: holds songs in music library
3. ``artists``: holds artists in music library
4. ``time``: holds timestamps of song plays separated into specific units

## Program Structure

The Apache Airflow DAG is shown below: <br>
<br>
![DAG_Project5](https://github.com/mhauck-FFM/Udacity_Data_Engineering_Projects/blob/master/Project_5/DAG_Airflow_project_5.png)

<br>
The project consists of the following files: <br>
```
.../airflow/dags:
  - sparkify_pipeline.py
```

The following files are included for this project:

  1. ``dl.cfg``, used to parse your AWS key and secret key
  2. ``etl.py``, main program to initialize the data lake and ELT process

Mandatory python modules to run the scripts:

- pyspark
- configparser
- datetime
- os

Note: The solution to the project has been developed locally using only the python files (``.py``) in Python 3.6.10.

#### Instructions to run the program

To run the program, start with the following first step:

1. Insert your AWS user credentials into ``dl.cfg`` at empty spaces:
    ```
    [AWS]
    key =
    secret =
    ```

Now, there are two options to run the program:

2. RUNNING THE SCRIPT: Run ``etl.py`` in the terminal using: ``python etl.py``

This method will use your local Spark installation or, if using an EMR notebook, the EMR clusters Spark installation. This may take some time to complete if run on your local machine!

2. SUBMIT TO EMR CLUSTER: Submit ``etl.py`` to the cluster by using the following in the master-node terminal: ``spark-submit etl.py --master yarn``

This method will use Spark on an EMR cluster. Make sure that the IAM role has read/write access to S3 and that the .py file is located on the master-node (SSH/SCP required). You might have to locate and adjust the spark-submit statement by using ``which spark-submit`` in the master-node terminal. Make sure the hadoop-aws.jar is present, otherwise there will be errors running the script.

Note: For successfully running the script, you may need to adjust the ``pwd`` of your terminal to the folder where both ``etl.py`` and ``dl.cfg`` are stored.
